{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi8gpuKw7SF9AHPJ5IZklg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21J41A0449/FineTuningLlama3B/blob/main/FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpR9OkQkPb1s"
      },
      "outputs": [],
      "source": [
        "# üìö Step 1: Import tools\n",
        "# Transformers = brings the brain (model)\n",
        "# Tokenizer = cuts sentences into Lego blocks\n",
        "# Datasets = helps us load our question-answer pairs\n",
        "# PEFT = lets us use the LoRA trick (train only a small part of brain)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# üß† Step 2: Pick which brain (model) we want\n",
        "# Here we choose the LLaMA-2-3B model (a 3 billion parameter brain)\n",
        "model_name = \"meta-llama/Llama-2-3b-hf\"\n",
        "\n",
        "# ‚úÇÔ∏è Step 3: Bring the Lego cutter (tokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üß© Step 4: Load the brain into memory\n",
        "# load_in_4bit=True makes the brain smaller (compressed) so it fits into our computer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"   # \"auto\" puts the brain on GPU if we have one\n",
        ")\n",
        "\n",
        "# üé® Step 5: LoRA setup\n",
        "# Instead of painting the whole wall, we only paint small patches\n",
        "# This saves paint (GPU memory) and time\n",
        "config = LoraConfig(\n",
        "    r=16,                          # size of LoRA patches\n",
        "    lora_alpha=32,                 # how strong the patches are\n",
        "    target_modules=[\"q_proj\",\"v_proj\"], # parts of the brain we will train\n",
        "    lora_dropout=0.05,              # chance to drop info (helps generalize)\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"          # \"Causal LM\" means text generation\n",
        ")\n",
        "\n",
        "# Put the LoRA patches onto our brain\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# üìÇ Step 6: Load our dataset\n",
        "# Example: a JSON file with question-answer pairs\n",
        "# Each entry looks like:\n",
        "# {\"instruction\": \"refund question\", \"input\": \"Can I get a refund?\", \"output\": \"Yes, within 14 days.\"}\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"train.json\", \"validation\": \"valid.json\"})\n",
        "\n",
        "# üèóÔ∏è Step 7: Convert text to Lego blocks (tokens)\n",
        "def format_example(example):\n",
        "    # We join instruction + input into a single question\n",
        "    question = example[\"instruction\"] + \"\\n\" + example[\"input\"]\n",
        "    # Answer is the output\n",
        "    answer = example[\"output\"]\n",
        "    # Tokenize both question and answer\n",
        "    return tokenizer(question + answer, truncation=True)\n",
        "\n",
        "# Apply this function to whole dataset\n",
        "tokenized_dataset = dataset.map(format_example)\n",
        "\n",
        "# üè´ Step 8: Set classroom rules (training arguments)\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,    # how many questions to read at once\n",
        "    per_device_eval_batch_size=4,     # same for test set\n",
        "    gradient_accumulation_steps=4,    # pretend we have a bigger batch\n",
        "    warmup_steps=50,                  # do some warmup before running fast\n",
        "    max_steps=500,                    # number of training steps (like study sessions)\n",
        "    learning_rate=2e-4,               # how fast to learn (too high = forgetful, too low = lazy)\n",
        "    fp16=True,                        # half precision = faster training\n",
        "    logging_steps=10,                 # how often to show progress\n",
        "    output_dir=\"./llama-finetuned\"    # where to save the trained brain\n",
        ")\n",
        "\n",
        "# üìñ Step 9: Create a trainer (teacher for the brain)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"]\n",
        ")\n",
        "\n",
        "# üèÉ Step 10: Start training!\n",
        "trainer.train()\n",
        "\n",
        "# üíæ Step 11: Save the trained brain\n",
        "model.save_pretrained(\"llama-3b-lora\")\n",
        "tokenizer.save_pretrained(\"llama-3b-lora\")\n",
        "\n",
        "# üéâ Done! Now our brain knows company rules (like refund policy)\n"
      ]
    }
  ]
}